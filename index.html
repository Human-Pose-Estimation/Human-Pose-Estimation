<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Human-pose-estimation by Human-Pose-Estimation</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Human-pose-estimation</h1>
      <h2 class="project-tagline"></h2>
      <a href="https://github.com/Human-Pose-Estimation/Human-Pose-Estimation" class="btn">View on GitHub</a>
      <a href="https://github.com/Human-Pose-Estimation/Human-Pose-Estimation/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/Human-Pose-Estimation/Human-Pose-Estimation/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <h1>
<a id="dataset-description" class="anchor" href="#dataset-description" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Dataset Description</h1>

<p style="text-indent:2em">Our kinect2 human gesture dataset(K2HGD) is composed about 100K depth images captured by kinect2. We transformed the depth value of the image to 0~255.K2 contains many different posees with different distances in many different scenes, including many challenging scenes and some unusual poses and occluded-poses.We used 60K for traning and 40K for testing in our paper <em><strong>Human Pose Estimation from Depth Data via Inference Embedded Multitask Learning</strong></em> </p>

<p>The K2HGD dataset contains many human activies in usual life and some unusual pose in very challenging scenes. We captured theses depth images using kinect2 because kinect2 has a better depth image sensor compared to other similar devices such as kinect1 or xtion.When using dept images for our model, we find that images captured by kinect2 is better for both accuracy and robustness. We use people to examine these depth images and for those  accuracy, we leave it as the groundtruth. for those not accurate, we only change the position of non-accurate part and leave others as kinect2 detected them. We used our own fast and  convenient java tools to adjust these joint positions, which can be very accurate with the location showing in each image.  </p>

<h1>
<a id="dataset-details" class="anchor" href="#dataset-details" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Dataset Details</h1>

<hr>

<p><a href="http://pan.baidu.com/s/1miaVe4o">http://pan.baidu.com/s/1miaVe4o</a></p>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/Human-Pose-Estimation/Human-Pose-Estimation">Human-pose-estimation</a> is maintained by <a href="https://github.com/Human-Pose-Estimation">Human-Pose-Estimation</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
