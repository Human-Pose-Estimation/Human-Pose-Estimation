<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Human-pose-estimation by Human-Pose-Estimation</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Human-pose-estimation</h1>
      <h2 class="project-tagline"></h2>
    </section>

    <section class="main-content">
<h1>
<a id="dataset-download" class="anchor" href="#dataset-download" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Dataset Download</h1>
<p>Below we will show some pictures of our dataset, but for more details of our dataset, you can download our part dataset demos or the full dataset.We split the full dataset to 2 parts: the first one is mainly normal gestures and some unusual and self-occlusion, and the second includes many unusual poses and self-occlusion. When training and tesing using our dataset, you should use both the first and the second to get better results.  </p>

<p> the part dataset demos:
[<a href="demoimages.zip">part demos</a>] </p>
<p>The first part full dataset:
[<a href="http://pan.baidu.com/s/1mitRWjI">Part1 Dataset</a>] , with the groundth location file:[<a href="groundtruthfiles/depth.txt">part1 groundtruth</a>]  </p>

<p>The second part full dataset:
[<a href="http://pan.baidu.com/s/1bpeBofl">Part2 Dataset</a>] ,  with the groundth location file:[<a href="groundtruthfiles/depth2.txt">part2 groundtruth</a>] </p>
<p>you can also use our part  groundtruth files . we split these files to 5 average parts and you can use them partly in ways such as K-fold cross-validation
<blockquote>
<p>1.[<a href="groundtruthfiles/depthpart1.txt">subfile1</a>]<br>
2.[<a href="groundtruthfiles/depthpart2.txt">subfile2</a>]<br>
3.[<a href="groundtruthfiles/depthpart3.txt">subfile3</a>]<br>
4.[<a href="groundtruthfiles/depthpart4.txt">subfile4</a>]<br>  
5.[<a href="groundtruthfiles/depthpart5.txt">subfile5</a>]<br>  
</p>
</blockquote>

<p>you can use our dataset to generate other groundtruth format such as heat-map, which describs the likeli-
hood of a joint occurring in each spatial location</p>

<h1>
<a id="dataset-description" class="anchor" href="#dataset-description" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Dataset Description</h1>

<p>Our kinect2 human gesture dataset(K2HGD) is composed about 100K depth images captured by kinect2 with resolution 512 x 424. We have 19 parts of human body and the order is :
<blockquote>
<p>1.Head<br>
2. Neck<br>
3.Spine_shoulder<br>
4.Spine_mid<br>
5.Spine_base<br>
6.Shoulder_right<br>
7.Elbow_right<br>
8.Wrist_right<br>
9.Hand_right<br>
10.Shoulder_left<br>
11.Elbow_left<br>
12.Wrist_left<br>
13.Hand_left<br>
14.Hip_right<br>
15.Knee_right<br>
16.Ankel_right<br>
17.Hip_left<br>
18.Knee_left<br>
19.Ankel_left</p>
</blockquote>
We transformed the depth value of the image to 0~255.K2HGD contains many different posees with different distances in many different scenes, including many challenging scenes and some unusual poses and occluded-poses. We offered the accurate groundtruth and we will tell you how we obtained the accurate groundtruth later </p>



<h1>
<a id="dataset-details" class="anchor" href="#dataset-details" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Dataset Details</h1>

<p>The dataset contains depth images captured by kinect2 with different people(boys and girls,about 30), different height,different distances and different challenging sceness(from simple scenes with only white wall to very challenging scenes which include many normal objects such as chairs). The dataset is aimed at detecting human gestures, so many depthimages are human usual life gestures.
Besides, We captured many unusual gestures and images with self-occlusion for the puspose of getting better detecting accuracy and generalization. the gestures our dataset contains has the following class(far more than that, We only show some standard of them because many gesture is combination of many gestures or you can't say clearly what class the belong):  </p>

<blockquote>
<p>1.Calling phone<br>
2.Drinking water<br>
3.Playing Tai Chi<br>
4.Walking around<br>
5.Giving a strecth<br>
6.Picking up something<br>
7.Driving car<br>
8.Wiping the glass<br>
9.Playing Basketball<br>
10.Puting on/Taking off spectacles<br>
11.setting-up exercises to music<br>
12.Sqatting down and stand up<br>
13.Many different normal jump actions<br>
14.Walking around and reading books<br>
15.Swing like birds<br>
16.Punching Attack<br>
17.Turning around<br>
18.Walking closer and far away<br>
19.Sitting in a chiar and turn around<br>
20.Other normal upper body movements(waving hands, Folding one's arms and so on)<br>
21.Other normal lower body movements(Kicking, Crossing legs and so on)<br>
22.unusual poses(bending with unusual angles,Serious occluson ,judo actions and so on)<br>
23.Poses with self-occlusion  (turning around,many body-occlusion actions , sitting behind objects and so on) </p>
</blockquote>

<p>The following is some examples of our dataset:
<!--<img src="datasetskeleton/00002550.png" alt="",style="left:50x;top:50px">-->
<!--<img src="datasetskeleton/00002793.png" alt="",style="left:50x;top:50px">-->
<!--<img src="datasetskeleton/00002999.png" alt="",style="left:50x;top:50px">-->
<!--<img src="datasetskeleton/00003345.png" alt="",style="left:50x;top:50px">-->
<!--<img src="datasetskeleton/00003368.png" alt="",style="left:50x;top:50px">-->
<!--<img src="datasetskeleton/00003972.png" alt="",style="left:50x;top:50px">-->
<!--<img src="datasetskeleton/00004056.png" alt="",style="left:50x;top:50px">-->
<!--<img src="datasetskeleton/00004244.png" alt="">-->
<!--<img src="datasetskeleton/00004430.png" alt="">-->
<!--<img src="datasetskeleton/00004826.png" alt="">-->
<!--<img src="datasetskeleton/00005104.png" alt="">-->
<!--<img src="datasetskeleton/00006454.png" alt="">-->
<!--<img src="datasetskeleton/00007389.png" alt="">-->
<!--<img src="datasetskeleton/00007918.png" alt="">-->
<!--<img src="datasetskeleton/00010250.png" alt="">-->
<!--<img src="datasetskeleton/00010352.png" alt="">-->
<!--<img src="datasetskeleton/00011249.png" alt="">-->
<!--<img src="datasetskeleton/00011481.png" alt="">-->
<!--<img src="datasetskeleton/00013244.png" alt="">-->
<!--<img src="datasetskeleton/00013311.png" alt="">-->
<!--<img src="datasetskeleton/00013358.png" alt="">-->
<!--<img src="datasetskeleton/00014174.png" alt="">-->
<!--<img src="datasetskeleton/00015344.png" alt="">-->
<!--<img src="datasetskeleton/00021926.png" alt="">-->
<!--<img src="datasetskeleton/00026835.png" alt="">-->
<!--<img src="datasetskeleton/00042249.png" alt="">-->
<!--<img src="datasetskeleton/00061083.png" alt="">-->
<!--<img src="datasetskeleton/00061775.png" alt="">-->
<!--<img src="datasetskeleton/00061784.png" alt="">-->
<!--<img src="datasetskeleton/00062639.png" alt="">-->
<!--<img src="datasetskeleton/00064502.png" alt="">-->
<!--<img src="datasetskeleton/00066345.png" alt="">-->
<!--<img src="datasetskeleton/00067293.png" alt="">-->
<!--<img src="datasetskeleton/00067785.png" alt="">-->
<!--<img src="datasetskeleton/00073329.png" alt="">-->
<!--<img src="datasetskeleton/00086992.png" alt="">-->
<!--<img src="datasetskeleton/00090665.png" alt="">-->
<!--<img src="datasetskeleton/00090910.png" alt="">-->
<!--<img src="datasetskeleton/00094126.png" alt="">-->
<table stype="float:left">
<tr><td><img src="datasetskeleton/00002550.png" style="width:200px"/>   </td><td><img src="datasetskeleton/00002793.png" style="width:200px"/></td><td><img src="datasetskeleton/00002999.png " style="width:200px"/>  </td><td><img src="datasetskeleton/00003345.png" style="width:200px"/></td><td><img src="datasetskeleton/00003368.png " style="width:200px"/>  </td><td><img src="datasetskeleton/00003972.png" style="width:200px"/></td><td><img src="datasetskeleton/00004056.png " style="width:200px"/>  </td><td><img src="datasetskeleton/00004244.png" style="width:200px"/></td></tr>
<tr><td><img src="datasetskeleton/00004430.png " style="width:200px"/>  </td><td><img src="datasetskeleton/00004826.png" style="width:200px"/></td><td><img src="datasetskeleton/00005104.png " style="width:200px"/>  </td><td><img src="datasetskeleton/00006454.png" style="width:200px"/></td><td><img src="datasetskeleton/00007389.png " style="width:200px"/>  </td><td><img src="datasetskeleton/00007918.png" style="width:200px"/></td><td><img src="datasetskeleton/00010250.png " style="width:200px"/>  </td><td><img src="datasetskeleton/00010352.png" style="width:200px"/></td></tr>
<tr><td><img src="datasetskeleton/00011249.png " style="width:200px"/>  </td><td><img src="datasetskeleton/00011481.png" style="width:200px"/></td><td><img src="datasetskeleton/00013244.png " style="width:200px"/>  </td><td><img src="datasetskeleton/00013311.png" style="width:200px"/></td><td><img src="datasetskeleton/00013358.png " style="width:200px"/>  </td><td><img src="datasetskeleton/00014174.png" style="width:200px"/></td><td><img src="datasetskeleton/00015344.png " style="width:200px"/>  </td><td><img src="datasetskeleton/00021926.png" style="width:200px"/></td></tr>
<tr><td><img src="datasetskeleton/00026835.png " style="width:200px"/>  </td><td><img src="datasetskeleton/00042249.png" style="width:200px"/></td><td><img src="datasetskeleton/00061083.png " style="width:200px"/>  </td><td><img src="datasetskeleton/00061775.png" style="width:200px"/></td><td><img src="datasetskeleton/00061784.png " style="width:200px"/>  </td><td><img src="datasetskeleton/00062639.png" style="width:200px"/></td><td><img src="datasetskeleton/00064502.png " style="width:200px"/>  </td><td><img src="datasetskeleton/00066345.png" style="width:200px"/></td></tr>
<tr><td><img src="datasetskeleton/00067293.png " style="width:200px"/>  </td><td><img src="datasetskeleton/00067785.png" style="width:200px"/></td><td><img src="datasetskeleton/00073329.png " style="width:200px"/>  </td><td><img src="datasetskeleton/00086992.png" style="width:200px"/></td><td><img src="datasetskeleton/00090665.png " style="width:200px"/>  </td><td><img src="datasetskeleton/00090910.png" style="width:200px"/></td><td><img src="datasetskeleton/00073329.png " style="width:200px"/>  </td><td><img src="datasetskeleton/00086992.png" style="width:200px"/></td></tr>
<tr><td><img src="datasetskeleton/00094126.png " style="width:200px"/>  </td></tr>
</table>
</p>

<p>The K2HGD dataset contains many human activies in usual life and some unusual posees in very challenging scenes. We captured theses depth images using kinect2 because kinect2 has a better depth image sensor compared to other similar devices such as kinect1 or xtion, which will be beneficial to learning both in accuracy and robustness. We use people to examine these depth images and for those  accuracy, we leave it as the groundtruth. for those not accurate, we only change the position of non-accurate part and leave others as kinect2 detected them. We used our own fast and  convenient java tools to adjust these joint positions, which can be very accurate with the location showing in each image(the joint will be shown when you put your mouse over it ,so you don't worry about confusing them. The location will show on the console windows so you can adjust the position at pixel level).<br>
<img src="testimagesdemo/show.png" alt="the java tool for adjustment"></p>




    </section>

  
  </body>
</html>
